{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akarsh-Kumar-2004/stock-finance/blob/main/WIEEEE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "doJ3GbIRDszf"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'kagglehub'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkagglehub\u001b[39;00m\n\u001b[32m      4\u001b[39m kagglehub.login()\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'kagglehub'"
          ]
        }
      ],
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Y95Y-M9LDszi"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'kagglehub' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# THEN FEEL FREE TO DELETE THIS CELL.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# NOTEBOOK.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m akarsh8_dataset_2_path = \u001b[43mkagglehub\u001b[49m.dataset_download(\u001b[33m'\u001b[39m\u001b[33makarsh8/dataset-2\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      8\u001b[39m akarsh8_dataset_1_path = kagglehub.dataset_download(\u001b[33m'\u001b[39m\u001b[33makarsh8/dataset-1\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mData source import complete.\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'kagglehub' is not defined"
          ]
        }
      ],
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "akarsh8_dataset_2_path = kagglehub.dataset_download('akarsh8/dataset-2')\n",
        "akarsh8_dataset_1_path = kagglehub.dataset_download('akarsh8/dataset-1')\n",
        "\n",
        "print('Data source import complete.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2025-04-04T23:06:18.473819Z",
          "iopub.status.busy": "2025-04-04T23:06:18.473529Z"
        },
        "id": "tTVtcwf3Dszi",
        "outputId": "60859f1f-2520-4e03-dca4-ffcf6e840cd5",
        "trusted": true
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Input, Dense, GRU, Dropout, BatchNormalization, SpatialDropout1D\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, GRU, Dropout, BatchNormalization, SpatialDropout1D\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import json\n",
        "from scipy.special import expit\n",
        "\n",
        "class EnhancedMarketPredictor:\n",
        "    def __init__(self):\n",
        "        self.feature_scaler = RobustScaler()\n",
        "        self.target_scaler = RobustScaler()\n",
        "        self.model = self._build_optimized_model()\n",
        "        self.market_regime = None\n",
        "\n",
        "    def _build_optimized_model(self):\n",
        "        \"\"\"Enhanced model architecture with balanced regularization\"\"\"\n",
        "        inputs = Input(shape=(None, 8))  # Updated for new features\n",
        "\n",
        "        x = GRU(128, return_sequences=True,\n",
        "                kernel_regularizer=l2(0.0005), recurrent_dropout=0.2)(inputs)\n",
        "        x = SpatialDropout1D(0.4)(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = GRU(64, kernel_regularizer=l2(0.0005))(x)\n",
        "        x = Dropout(0.5)(x)\n",
        "        x = Dense(32, activation='relu', kernel_regularizer=l2(0.0005))(x)\n",
        "        output = Dense(1, activation='linear')(x)\n",
        "\n",
        "        model = Model(inputs=inputs, outputs=output)\n",
        "        model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(3e-5),\n",
        "            loss=self._dynamic_loss,\n",
        "            metrics=[self._safe_direction_accuracy, 'mae']\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    def _dynamic_loss(self, y_true, y_pred):\n",
        "        \"\"\"Consistent Huber loss for stability\"\"\"\n",
        "        return tf.keras.losses.huber(y_true, y_pred)\n",
        "\n",
        "    def _safe_direction_accuracy(self, y_true, y_pred):\n",
        "        \"\"\"Robust directional accuracy with edge case handling\"\"\"\n",
        "        y_true_diff = y_true[1:] - y_true[:-1]\n",
        "        y_pred_diff = y_pred[1:] - y_pred[:-1]\n",
        "        return tf.reduce_mean(tf.cast(\n",
        "            tf.equal(tf.sign(y_true_diff),\n",
        "            tf.sign(y_pred_diff)\n",
        "        ), tf.float32))\n",
        "\n",
        "    def _detect_market_regime(self, df):\n",
        "        \"\"\"Enhanced regime detection with volatility clustering\"\"\"\n",
        "        returns = df['close'].pct_change().dropna()\n",
        "        volatility = returns.rolling(10).std().dropna()\n",
        "        self.market_regime = 'downturn' if volatility.mean() > 0.015 else 'normal'\n",
        "        print(f\"Detected market regime: {self.market_regime}\")\n",
        "\n",
        "    def _create_features(self, df):\n",
        "        \"\"\"Enhanced feature engineering with momentum indicators\"\"\"\n",
        "        df['returns'] = df['close'].pct_change().clip(-0.3, 0.3)\n",
        "        df['momentum_3'] = df['close'].pct_change(3)\n",
        "        df['momentum_7'] = df['close'].pct_change(7)\n",
        "\n",
        "        # Volatility features\n",
        "        windows = [3, 7] if self.market_regime == 'downturn' else [10, 20]\n",
        "        for w in windows:\n",
        "            df[f'vol_{w}'] = df['returns'].rolling(w).std().fillna(0)\n",
        "            df[f'mdd_{w}'] = (df['close'].rolling(w).max() - df['close']) / df['close'].rolling(w).max()\n",
        "\n",
        "        return df.ffill().bfill().fillna(0)\n",
        "\n",
        "    def load_and_prepare(self, filepaths):\n",
        "        \"\"\"Load and prepare data from filepaths\"\"\"\n",
        "        dfs = []\n",
        "        for path in filepaths:\n",
        "            df = pd.read_csv(path)\n",
        "            df.columns = df.columns.str.lower().str.strip()\n",
        "            dfs.append(df)\n",
        "\n",
        "        combined = pd.concat(dfs).sort_index()\n",
        "        self._detect_market_regime(combined)\n",
        "        return self._create_features(combined)\n",
        "\n",
        "    def prepare_sequences(self, df, lookback=30):\n",
        "        \"\"\"Leakage-proof sequence preparation\"\"\"\n",
        "        features = ['close', 'vol_3', 'vol_7', 'mdd_3', 'mdd_7',\n",
        "                  'momentum_3', 'momentum_7', 'returns']\n",
        "\n",
        "        # Split before scaling\n",
        "        split_idx = int((len(df) - lookback) * 0.8)\n",
        "        train_df = df.iloc[:split_idx + lookback]\n",
        "        test_df = df.iloc[split_idx:]\n",
        "\n",
        "        # Fit scalers only on training data\n",
        "        self.feature_scaler.fit(train_df[features])\n",
        "        self.target_scaler.fit(train_df[['close']])\n",
        "\n",
        "        # Transform both sets\n",
        "        train_features = self.feature_scaler.transform(train_df[features])\n",
        "        test_features = self.feature_scaler.transform(test_df[features])\n",
        "        train_target = self.target_scaler.transform(train_df[['close']])\n",
        "        test_target = self.target_scaler.transform(test_df[['close']])\n",
        "\n",
        "        # Create sequences\n",
        "        def create_sequences(features, target):\n",
        "            X, y = [], []\n",
        "            for i in range(len(features) - lookback):\n",
        "                X.append(features[i:i+lookback])\n",
        "                y.append(target[i+lookback])\n",
        "            return np.array(X), np.array(y)\n",
        "\n",
        "        X_train, y_train = create_sequences(train_features, train_target)\n",
        "        X_val, y_val = create_sequences(test_features, test_target)\n",
        "\n",
        "        return X_train, y_train, X_val, y_val\n",
        "\n",
        "    def train(self, X_train, y_train, X_val, y_val):\n",
        "        \"\"\"Optimized training configuration\"\"\"\n",
        "        early_stop = EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=10,\n",
        "            restore_best_weights=True,\n",
        "            min_delta=0.0005\n",
        "        )\n",
        "\n",
        "        lr_reducer = ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.2,\n",
        "            patience=3,\n",
        "            verbose=1,\n",
        "            min_lr=1e-6\n",
        "        )\n",
        "\n",
        "        history = self.model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_data=(X_val, y_val),\n",
        "            epochs=20,\n",
        "            batch_size=128,\n",
        "            callbacks=[early_stop, lr_reducer],\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # Analysis of best epoch\n",
        "        best_epoch = np.argmin(history.history['val_loss'])\n",
        "        print(f\"\\nBest Model at Epoch {best_epoch+1}:\")\n",
        "        print(f\"Train Loss: {history.history['loss'][best_epoch]:.4f}\")\n",
        "        print(f\"Val Loss: {history.history['val_loss'][best_epoch]:.4f}\")\n",
        "        print(f\"Direction Accuracy: {history.history['val_safe_direction_accuracy'][best_epoch]:.4f}\")\n",
        "\n",
        "        return history\n",
        "\n",
        "    def predict_directions(self, X_test):\n",
        "        \"\"\"Predict whether stock will go up or down with confidence scores\"\"\"\n",
        "        y_pred = self.model.predict(X_test)\n",
        "\n",
        "        directions = []\n",
        "        confidences = []\n",
        "\n",
        "        for i in range(1, len(y_pred)):\n",
        "            direction = 1 if y_pred[i] > y_pred[i-1] else 0\n",
        "            confidence = expit(np.abs(y_pred[i] - y_pred[i-1]))\n",
        "\n",
        "            directions.append(direction)\n",
        "            confidences.append(float(confidence))\n",
        "\n",
        "        return directions, confidences\n",
        "\n",
        "    def save_direction_predictions(self, X_test, filename='direction_predictions.json'):\n",
        "        \"\"\"Save direction predictions to JSON file\"\"\"\n",
        "        directions, confidences = self.predict_directions(X_test)\n",
        "\n",
        "        predictions = {\n",
        "            \"predictions\": [\n",
        "                {\n",
        "                    \"direction\": \"up\" if direction == 1 else \"down\",\n",
        "                    \"confidence\": confidence,\n",
        "                    \"next_period\": i+1\n",
        "                }\n",
        "                for i, (direction, confidence) in enumerate(zip(directions, confidences))\n",
        "            ],\n",
        "            \"metadata\": {\n",
        "                \"model_type\": \"GRU\",\n",
        "                \"market_regime\": self.market_regime,\n",
        "                \"prediction_horizon\": \"next_period\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "        with open(filename, 'w') as f:\n",
        "            json.dump(predictions, f, indent=4)\n",
        "\n",
        "        print(f\"Direction predictions saved to {filename}\")\n",
        "        return predictions\n",
        "\n",
        "# Main Execution\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        predictor = EnhancedMarketPredictor()\n",
        "        data = predictor.load_and_prepare([\n",
        "            \"/kaggle/input/dataset-1/Dataset1.csv\",\n",
        "            \"/kaggle/input/dataset-2/Dataset2.csv\"\n",
        "        ])\n",
        "\n",
        "        # Prepare data with built-in validation split\n",
        "        X_train, y_train, X_val, y_val = predictor.prepare_sequences(data)\n",
        "\n",
        "        # Train and print metrics\n",
        "        history = predictor.train(X_train, y_train, X_val, y_val)\n",
        "\n",
        "        # Save direction predictions\n",
        "        predictions = predictor.save_direction_predictions(X_val)\n",
        "\n",
        "        # Plot training history\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(history.history['safe_direction_accuracy'], label='Train')\n",
        "        plt.plot(history.history['val_safe_direction_accuracy'], label='Validation')\n",
        "        plt.title('Direction Accuracy')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(history.history['mae'], label='Train')\n",
        "        plt.plot(history.history['val_mae'], label='Validation')\n",
        "        plt.title('Mean Absolute Error')\n",
        "        plt.ylabel('MAE')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "name": "WIEEEE",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 7051932,
          "sourceId": 11279593,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 7051934,
          "sourceId": 11279597,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30919,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
